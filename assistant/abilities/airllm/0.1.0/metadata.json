{
    "id": "airllm",
    "name": "AirLLM",
    "versions": {
        "package": "0.1.0",
        "product": "2.8"
    },
    "description": "AirLLM optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card.",
    "author": {
        "name": "Gavin Li",
        "url": "https://github.com/lyogavin/Anima/tree/main/air_llm"
    },
    "maintainer": {
        "name": "Sam Johnston",
        "email": "samj@samj.net",
        "url": "http://samjohnston.org"
    },
    "language": "python",
    "scripts": {
        "start": "main.py"
    },
    "license": "Apache-2.0",
    "dependencies": [
        {
            "type": "python",
            "id": "airllm",
            "name": "AirLLM",
            "versions":
                {
                    "required": ">=2.8"
                }
        },
        {
            "type": "python",
            "id": "mlx",
            "name": "MLX",
            "versions":
                {
                    "required": ">=0.12"
                }
        }
    ]
}
